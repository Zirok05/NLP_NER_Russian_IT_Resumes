{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c652ac32-217e-485f-99bf-c7c5dc339300",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6e2623b-c202-4fdf-87ff-8d1550c9bccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ner_pipelines(model_paths):\n",
    "    \"\"\"–°–æ–∑–¥–∞–µ—Ç NER –ø–∞–π–ø–ª–∞–π–Ω—ã –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π\"\"\"\n",
    "    pipelines = {}\n",
    "    \n",
    "    for group_name, path in model_paths.items():\n",
    "        print(f\"–°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–π–ø–ª–∞–π–Ω–∞ –¥–ª—è {group_name} –º–æ–¥–µ–ª–∏...\")\n",
    "        \n",
    "        ner_pipe = pipeline(\n",
    "            \"ner\",\n",
    "            model=path,\n",
    "            tokenizer=path,\n",
    "            aggregation_strategy=\"first\",\n",
    "            stride=64,\n",
    "            device= \"mps\" if torch.backends.mps.is_available() else \"cpu\", # –¥–ª—è Apple Silicon\n",
    "        )\n",
    "        \n",
    "        pipelines[group_name] = ner_pipe\n",
    "    \n",
    "    return pipelines\n",
    "\n",
    "def process_text_with_pipelines(text, pipelines):\n",
    "    \"\"\"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –≤—Å–µ–º–∏ –ø–∞–π–ø–ª–∞–π–Ω–∞–º–∏ –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\"\"\"\n",
    "    all_entities = []\n",
    "    \n",
    "    for group_name, ner_pipe in pipelines.items():\n",
    "        try:\n",
    "            entities = ner_pipe(text)\n",
    "            \n",
    "            # –î–æ–±–∞–≤–ª—è–µ–º —Å—É—â–Ω–æ—Å—Ç–∏ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –≥—Ä—É–ø–ø–µ\n",
    "            for entity in entities:\n",
    "                entity['model_group'] = group_name\n",
    "                all_entities.append(entity)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"–û—à–∏–±–∫–∞ –≤ –º–æ–¥–µ–ª–∏ {group_name}: {e}\")\n",
    "    \n",
    "    return all_entities\n",
    "\n",
    "def process_all_resumes_pipeline(resume_texts, model_paths):\n",
    "    \"\"\"–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö —Ä–µ–∑—é–º–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º pipeline\"\"\"\n",
    "    \n",
    "    print(\"–°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–π–ø–ª–∞–π–Ω–æ–≤...\")\n",
    "    pipelines = create_ner_pipelines(model_paths)\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    print(f\"–û–±—Ä–∞–±–æ—Ç–∫–∞ {len(resume_texts)} —Ä–µ–∑—é–º–µ —Å –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–º–∏ —Å–∫–æ–ª—å–∑—è—â–∏–º–∏ –æ–∫–Ω–∞–º–∏\")\n",
    "    \n",
    "    for i, text in enumerate(resume_texts):\n",
    "        if i % 10 == 0:\n",
    "            print(f\"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i}/{len(resume_texts)} —Ä–µ–∑—é–º–µ\")\n",
    "        \n",
    "        if not text or str(text).strip() == \"\":\n",
    "            all_results.append({\n",
    "                'text': text,\n",
    "                'entities': [],\n",
    "                'entity_count': 0,\n",
    "                'overall_confidence': 0.0\n",
    "            })\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            entities = process_text_with_pipelines(str(text), pipelines)\n",
    "            \n",
    "            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π —Ñ–æ—Ä–º–∞—Ç \n",
    "            formatted_entities = []\n",
    "            for entity in entities:\n",
    "                formatted_entities.append({\n",
    "                    'start': entity['start'],\n",
    "                    'end': entity['end'],\n",
    "                    'label': entity['entity_group'],\n",
    "                    'text': entity['word'],\n",
    "                    'confidence': float(entity['score']),  \n",
    "                    'model_group': entity['model_group']\n",
    "                })\n",
    "            \n",
    "            # –í—ã—á–∏—Å–ª—è–µ–º –æ–±—â—É—é —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç—É\n",
    "            overall_confidence = 0.0\n",
    "            if formatted_entities:\n",
    "                overall_confidence = sum(e['confidence'] for e in formatted_entities) / len(formatted_entities)\n",
    "            \n",
    "            all_results.append({\n",
    "                'text': text,\n",
    "                'entities': formatted_entities,\n",
    "                'entity_count': len(formatted_entities),\n",
    "                'overall_confidence': overall_confidence\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"–û—à–∏–±–∫–∞ —Å —Ä–µ–∑—é–º–µ {i}: {e}\")\n",
    "            all_results.append({\n",
    "                'text': text,\n",
    "                'entities': [],\n",
    "                'entity_count': 0,\n",
    "                'overall_confidence': 0.0\n",
    "            })\n",
    "    \n",
    "    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏\n",
    "    all_results_sorted = sorted(all_results, key=lambda x: x['overall_confidence'], reverse=True)\n",
    "    \n",
    "    print(f\"–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: –æ—Ç {all_results_sorted[-1]['overall_confidence']:.3f} –¥–æ {all_results_sorted[0]['overall_confidence']:.3f}\")\n",
    "    \n",
    "    return all_results_sorted\n",
    "\n",
    "def export_to_label_studio_pipeline(predictions, output_file=\"label_studio_pipeline.json\"):\n",
    "    \"\"\"–≠–∫—Å–ø–æ—Ä—Ç –≤ Label Studio —Å confidence scores\"\"\"\n",
    "    label_studio_tasks = []\n",
    "    \n",
    "    for i, pred in enumerate(predictions):\n",
    "        text = pred['text']\n",
    "        entities = pred['entities']\n",
    "        \n",
    "        # –°–æ–∑–¥–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è Label Studio\n",
    "        results = []\n",
    "        for entity in entities:\n",
    "            result = {\n",
    "                \"value\": {\n",
    "                    \"start\": entity['start'],\n",
    "                    \"end\": entity['end'], \n",
    "                    \"text\": entity['text'],\n",
    "                    \"labels\": [entity['label']]\n",
    "                },\n",
    "                \"id\": f\"pred_{i}_{entity['start']}_{entity['end']}\",\n",
    "                \"from_name\": \"label\",\n",
    "                \"to_name\": \"text\", \n",
    "                \"type\": \"labels\",\n",
    "                \"score\": entity['confidence']  # —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è –∫–∞–∂–¥–æ–π —Å—É—â–Ω–æ—Å—Ç–∏\n",
    "            }\n",
    "            results.append(result)\n",
    "        \n",
    "        # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á—É –¥–ª—è Label Studio\n",
    "        task = {\n",
    "            \"data\": {\n",
    "                \"text\": str(text),\n",
    "                \"id\": i + 1,\n",
    "                \"score\": pred['overall_confidence'] # –û–±—â–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç—É \n",
    "            },\n",
    "            \"predictions\": [{\n",
    "                \"result\": results,\n",
    "                \"model_version\": \"pipeline-3-model-ensemble\",\n",
    "                \"score\": pred['overall_confidence']  # –û–±—â–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç—É \n",
    "            }]\n",
    "        }\n",
    "        \n",
    "        label_studio_tasks.append(task)\n",
    "    \n",
    "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(label_studio_tasks, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ {len(label_studio_tasks)} –∑–∞–¥–∞—á\")\n",
    "    return label_studio_tasks\n",
    "\n",
    "def save_predictions_csv_pipeline(predictions, output_file=\"predictions_pipeline.csv\"):\n",
    "    \"\"\"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ CSV —Å confidence scores\"\"\"\n",
    "    rows = []\n",
    "    for i, pred in enumerate(predictions):\n",
    "        text_preview = str(pred['text'])[:100] + \"...\" if len(str(pred['text'])) > 100 else str(pred['text'])\n",
    "        \n",
    "        entities_str = \"\"\n",
    "        if pred['entities']:\n",
    "            entities_list = [f\"{e['text']} ({e['label']}:{e['confidence']:.3f})\" for e in pred['entities']]\n",
    "            entities_str = \"; \".join(entities_list)\n",
    "        \n",
    "        rows.append({\n",
    "            'sort_order': i + 1,\n",
    "            'text_preview': text_preview,\n",
    "            'entities_found': entities_str,\n",
    "            'entity_count': pred['entity_count'],\n",
    "            'overall_confidence': f\"{pred['overall_confidence']:.3f}\"  # –æ–±—â–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "    print(f\"CSV —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_file}\")\n",
    "\n",
    "def analyze_confidence(predictions):\n",
    "    \"\"\"–ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è confidence scores\"\"\"\n",
    "    all_confidences = []\n",
    "    for pred in predictions:\n",
    "        for entity in pred['entities']:\n",
    "            all_confidences.append(entity['confidence'])\n",
    "    \n",
    "    if all_confidences:\n",
    "        print(f\"–ê–Ω–∞–ª–∏–∑ confidence scores:\")\n",
    "        print(f\"–°—Ä–µ–¥–Ω–µ–µ: {sum(all_confidences) / len(all_confidences):.3f}\")\n",
    "        print(f\"–ú–µ–¥–∏–∞–Ω–∞: {sorted(all_confidences)[len(all_confidences)//2]:.3f}\")\n",
    "        print(f\"Min: {min(all_confidences):.3f}\")\n",
    "        print(f\"Max: {max(all_confidences):.3f}\")\n",
    "        \n",
    "        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞–º\n",
    "        ranges = [(0, 0.5), (0.5, 0.7), (0.7, 0.9), (0.9, 1.0)]\n",
    "        for r_min, r_max in ranges:\n",
    "            count = sum(1 for c in all_confidences if r_min <= c < r_max)\n",
    "            print(f\"{r_min:.1f}-{r_max:.1f}: {count} —Å—É—â–Ω–æ—Å—Ç–µ–π ({count/len(all_confidences)*100:.1f}%)\")\n",
    "\n",
    "# –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è\n",
    "def full_pipeline(resume_texts, model_paths):\n",
    "    print(\"–ó–∞–ø—É—Å–∫ –ø–∞–π–ø–ª–∞–π–Ω–∞...\")\n",
    "    \n",
    "    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å pipeline\n",
    "    all_results = process_all_resumes_pipeline(resume_texts, model_paths)\n",
    "    \n",
    "    # –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞\n",
    "    analyze_confidence(all_results)\n",
    "    \n",
    "    # –≠–∫—Å–ø–æ—Ä—Ç\n",
    "    export_to_label_studio_pipeline(all_results, \"final_pipeline_predictions.json\")\n",
    "    save_predictions_csv_pipeline(all_results, \"predictions_pipeline.csv\")\n",
    "    print(\"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d02f33e-479f-43c4-98f3-88a60c43d8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ –ó–∞–ø—É—Å–∫ –Ω–æ–≤–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞ —Å –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–º–∏ —Å–∫–æ–ª—å–∑—è—â–∏–º–∏ –æ–∫–Ω–∞–º–∏...\n",
      "üîÑ –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–π–ø–ª–∞–π–Ω–æ–≤...\n",
      "üîÑ –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–π–ø–ª–∞–π–Ω–∞ –¥–ª—è group1 –º–æ–¥–µ–ª–∏...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n",
      "Device set to use mps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–π–ø–ª–∞–π–Ω–∞ –¥–ª—è group2 –º–æ–¥–µ–ª–∏...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–π–ø–ª–∞–π–Ω–∞ –¥–ª—è group3 –º–æ–¥–µ–ª–∏...\n",
      "üöÄ –û–±—Ä–∞–±–æ—Ç–∫–∞ 100 —Ä–µ–∑—é–º–µ —Å –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–º–∏ —Å–∫–æ–ª—å–∑—è—â–∏–º–∏ –æ–∫–Ω–∞–º–∏\n",
      "üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 0/100 —Ä–µ–∑—é–º–µ\n",
      "üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 10/100 —Ä–µ–∑—é–º–µ\n",
      "üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 20/100 —Ä–µ–∑—é–º–µ\n",
      "üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 30/100 —Ä–µ–∑—é–º–µ\n",
      "üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 40/100 —Ä–µ–∑—é–º–µ\n",
      "üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 50/100 —Ä–µ–∑—é–º–µ\n",
      "üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 60/100 —Ä–µ–∑—é–º–µ\n",
      "üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 70/100 —Ä–µ–∑—é–º–µ\n",
      "üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 80/100 —Ä–µ–∑—é–º–µ\n",
      "üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 90/100 —Ä–µ–∑—é–º–µ\n",
      "‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!\n",
      "üìà –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: –æ—Ç 0.882 –¥–æ 1.000\n",
      "üìä –ê–Ω–∞–ª–∏–∑ confidence scores:\n",
      "   –°—Ä–µ–¥–Ω–µ–µ: 0.953\n",
      "   –ú–µ–¥–∏–∞–Ω–∞: 1.000\n",
      "   Min: 0.291\n",
      "   Max: 1.000\n",
      "   0.0-0.5: 89 —Å—É—â–Ω–æ—Å—Ç–µ–π (1.0%)\n",
      "   0.5-0.7: 452 —Å—É—â–Ω–æ—Å—Ç–µ–π (5.2%)\n",
      "   0.7-0.9: 692 —Å—É—â–Ω–æ—Å—Ç–µ–π (7.9%)\n",
      "   0.9-1.0: 7466 —Å—É—â–Ω–æ—Å—Ç–µ–π (85.3%)\n",
      "‚úÖ –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ 100 –∑–∞–¥–∞—á —Å confidence scores\n",
      "‚úÖ CSV —Å–æ—Ö—Ä–∞–Ω–µ–Ω: predictions_pipeline.csv\n",
      "üéâ –ù–û–í–´–ô –ü–ê–ô–ü–õ–ê–ô–ù –ó–ê–í–ï–†–®–ï–ù! –° –Ω–∞—Å—Ç–æ—è—â–∏–º–∏ confidence scores!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "FOLDER_PATH = '../models/'\n",
    "resumes_df = pd.read_csv('../datasets/test_batch_resumes.csv', index_col=0)\n",
    "unlabeled_resumes = resumes_df['description'].dropna().tolist()\n",
    "model_paths = {\n",
    "    'group1': FOLDER_PATH + 'model_1_final',\n",
    "    'group2': FOLDER_PATH + 'model_2_final', \n",
    "    'group3': FOLDER_PATH + 'model_3_final'\n",
    "}\n",
    "\n",
    "final_results = full_pipeline(unlabeled_resumes, model_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "833ff6b2-0d94-4fed-9fdd-b16dce34e4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# FOLDER_PATH = '/Users/artemzmailov/Desktop/–í–ö–†/Learning/models_307_resumes_stride_64/'\n",
    "# resumes_df = pd.read_csv('/Users/artemzmailov/Desktop/–í–ö–†/test_batch_resumes.csv', index_col=0)\n",
    "# unlabeled_resumes = resumes_df['description'].dropna().tolist()\n",
    "# model_paths = {\n",
    "#     'group1': FOLDER_PATH + 'model_1_final',\n",
    "# }\n",
    "\n",
    "# final_results = full_pipeline_new(unlabeled_resumes, model_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "658c2f49-1044-4402-8eab-616f46a98b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ó–∞–ø—É—Å–∫ –ø–∞–π–ø–ª–∞–π–Ω–∞...\n",
      "–°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–π–ø–ª–∞–π–Ω–æ–≤...\n",
      "–°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–π–ø–ª–∞–π–Ω–∞ –¥–ª—è group1 –º–æ–¥–µ–ª–∏...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n",
      "Device set to use mps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–π–ø–ª–∞–π–Ω–∞ –¥–ª—è group2 –º–æ–¥–µ–ª–∏...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–π–ø–ª–∞–π–Ω–∞ –¥–ª—è group3 –º–æ–¥–µ–ª–∏...\n",
      "–û–±—Ä–∞–±–æ—Ç–∫–∞ 117 —Ä–µ–∑—é–º–µ —Å –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–º–∏ —Å–∫–æ–ª—å–∑—è—â–∏–º–∏ –æ–∫–Ω–∞–º–∏\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 0/117 —Ä–µ–∑—é–º–µ\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 10/117 —Ä–µ–∑—é–º–µ\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 20/117 —Ä–µ–∑—é–º–µ\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 30/117 —Ä–µ–∑—é–º–µ\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 40/117 —Ä–µ–∑—é–º–µ\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 50/117 —Ä–µ–∑—é–º–µ\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 60/117 —Ä–µ–∑—é–º–µ\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 70/117 —Ä–µ–∑—é–º–µ\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 80/117 —Ä–µ–∑—é–º–µ\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 90/117 —Ä–µ–∑—é–º–µ\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 100/117 —Ä–µ–∑—é–º–µ\n",
      "–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 110/117 —Ä–µ–∑—é–º–µ\n",
      "–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: –æ—Ç 0.832 –¥–æ 0.988\n",
      "–ê–Ω–∞–ª–∏–∑ confidence scores:\n",
      "–°—Ä–µ–¥–Ω–µ–µ: 0.944\n",
      "–ú–µ–¥–∏–∞–Ω–∞: 0.999\n",
      "Min: 0.311\n",
      "Max: 1.000\n",
      "0.0-0.5: 97 —Å—É—â–Ω–æ—Å—Ç–µ–π (0.6%)\n",
      "0.5-0.7: 1009 —Å—É—â–Ω–æ—Å—Ç–µ–π (6.7%)\n",
      "0.7-0.9: 1504 —Å—É—â–Ω–æ—Å—Ç–µ–π (10.0%)\n",
      "0.9-1.0: 12424 —Å—É—â–Ω–æ—Å—Ç–µ–π (82.5%)\n",
      "–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ 117 –∑–∞–¥–∞—á\n",
      "CSV —Å–æ—Ö—Ä–∞–Ω–µ–Ω: predictions_pipeline.csv\n",
      "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_json('../hh_ru_API_parser/resumes_json/for_label_studio/label_studio_all_117.json')\n",
    "test_df['text'] = test_df['data'].apply(lambda x: x['text'])\n",
    "text_lst = test_df['text'].to_list()\n",
    "FOLDER_PATH = '../models/'\n",
    "model_paths = {\n",
    "    'group1': FOLDER_PATH + 'model_1_final',\n",
    "    'group2': FOLDER_PATH + 'model_2_final', \n",
    "    'group3': FOLDER_PATH + 'model_3_final'\n",
    "}\n",
    "\n",
    "final_results = full_pipeline(text_lst, model_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30d5a14-4745-41c2-83aa-2e11e6ca3605",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
