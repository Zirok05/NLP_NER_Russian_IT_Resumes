{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09c0e696-3bd7-4629-a082-19927d0e0730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Создан датасет group1_dataset.jsonl с 307 примерами\n",
      "Статистика меток для group1:\n",
      "   B-DEGREE: 559\n",
      "   B-LINKS: 1638\n",
      "   B-LOCATION: 1599\n",
      "   B-METRICS: 425\n",
      "   B-POSITIONS: 1826\n",
      "   B-TIME: 3783\n",
      "   I-DEGREE: 1481\n",
      "   I-LINKS: 22789\n",
      "   I-LOCATION: 1117\n",
      "   I-METRICS: 1827\n",
      "   I-POSITIONS: 8522\n",
      "   I-TIME: 13070\n",
      "   O: 310168\n",
      "Создан датасет group2_dataset.jsonl с 307 примерами\n",
      "Статистика меток для group2:\n",
      "   B-COMPANIES: 2105\n",
      "   B-NAME: 437\n",
      "   B-TECHNOLOGIES: 12007\n",
      "   I-COMPANIES: 10251\n",
      "   I-NAME: 1787\n",
      "   I-TECHNOLOGIES: 19284\n",
      "   O: 322924\n",
      "Создан датасет group3_dataset.jsonl с 307 примерами\n",
      "Статистика меток для group3:\n",
      "   B-ACHIEVEMENTS: 1303\n",
      "   B-CONTACTS: 199\n",
      "   B-EDUCATION: 1151\n",
      "   B-PROJECTS: 1051\n",
      "   B-RESPONSIBILITIES: 5793\n",
      "   B-SKILLS: 1194\n",
      "   I-ACHIEVEMENTS: 31530\n",
      "   I-CONTACTS: 2828\n",
      "   I-EDUCATION: 14600\n",
      "   I-PROJECTS: 25200\n",
      "   I-RESPONSIBILITIES: 103769\n",
      "   I-SKILLS: 14284\n",
      "   O: 165902\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Gherman/bert-base-NER-Russian\")\n",
    "\n",
    "\n",
    "# Загрузка исходных данных\n",
    "FOLDER_PATH = '../datasets/'\n",
    "with open(FOLDER_PATH + '307_labeled_resumes_no_duplicates.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Группы сущностей\n",
    "GROUP_1 = ['DEGREE', 'LOCATION', 'TIME', 'LINKS', 'METRICS', 'POSITIONS']\n",
    "GROUP_2 = ['COMPANIES', 'TECHNOLOGIES', 'NAME']\n",
    "GROUP_3 = ['PROJECTS', 'ACHIEVEMENTS', 'RESPONSIBILITIES', 'SKILLS', 'EDUCATION', 'CONTACTS']\n",
    "\n",
    "def extract_annotations_with_text(item):\n",
    "    \"\"\"Извлекает аннотации с текстом\"\"\"\n",
    "    \n",
    "    annotations = []\n",
    "    text = item['data']['text']\n",
    "    \n",
    "    for ann in item['annotations']:\n",
    "        for result in ann['result']:\n",
    "            if 'value' in result:\n",
    "                value = result['value']\n",
    "                start = value['start']\n",
    "                end = value['end']\n",
    "                label = value['labels'][0]\n",
    "                annotated_text = text[start:end]\n",
    "                \n",
    "                annotations.append({\n",
    "                    'start': start,\n",
    "                    'end': end,\n",
    "                    'label': label,\n",
    "                    'text': annotated_text\n",
    "                })\n",
    "    \n",
    "    return annotations, text\n",
    "\n",
    "\n",
    "def create_ner_sequence(text, annotations, target_labels, tokenizer):\n",
    "    \"\"\"Создает NER последовательность с BERT-токенизацией для всех сущностей и приведением к IOB формату \"\"\"\n",
    "    \n",
    "    annotations = sorted(annotations, key=lambda x: x['start'])\n",
    "    \n",
    "    # Создаем карту аннотаций для быстрого поиска\n",
    "    annotation_map = {}\n",
    "    for ann in annotations:\n",
    "        if ann['label'] in target_labels:\n",
    "            for pos in range(ann['start'], ann['end']):\n",
    "                annotation_map[pos] = ann\n",
    "    \n",
    "    tokens = []\n",
    "    labels = []\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(text):\n",
    "        if text[i].isspace():\n",
    "            i += 1\n",
    "            continue\n",
    "            \n",
    "        # Проверяем, начинается ли здесь аннотация\n",
    "        if i in annotation_map:\n",
    "            current_ann = annotation_map[i]\n",
    "            annotated_text = current_ann['text']\n",
    "            \n",
    "            # Токенезируем как BERT для любой сущности\n",
    "            sub_tokens = tokenizer.tokenize(annotated_text)\n",
    "            \n",
    "            if not sub_tokens:\n",
    "                sub_tokens = [annotated_text]\n",
    "            \n",
    "            # Добавляем с правильными BIO-метками\n",
    "            for sub_idx, sub_token in enumerate(sub_tokens):\n",
    "                tokens.append(sub_token)\n",
    "                if sub_idx == 0:\n",
    "                    labels.append(f'B-{current_ann[\"label\"]}')\n",
    "                else:\n",
    "                    labels.append(f'I-{current_ann[\"label\"]}')\n",
    "            \n",
    "            i = current_ann['end']\n",
    "            \n",
    "        else:\n",
    "            # Не аннотированный текст - тоже токенизируем через BERT\n",
    "            # Находим границу неаннотированного текста\n",
    "            j = i\n",
    "            while j < len(text) and j not in annotation_map and not text[j].isspace():\n",
    "                j += 1\n",
    "            \n",
    "            if j > i:\n",
    "                non_annotated_text = text[i:j]\n",
    "                sub_tokens = tokenizer.tokenize(non_annotated_text)\n",
    "                \n",
    "                if not sub_tokens:\n",
    "                    sub_tokens = [non_annotated_text]\n",
    "                \n",
    "                for sub_token in sub_tokens:\n",
    "                    tokens.append(sub_token)\n",
    "                    labels.append('O')\n",
    "                \n",
    "                i = j\n",
    "            else:\n",
    "                i += 1\n",
    "    \n",
    "    return tokens, labels\n",
    "\n",
    "def create_dataset_for_group(data,group_labels, group_name):\n",
    "    \"\"\"Создает датасет для конкретной группы сущностей\"\"\"\n",
    "    dataset = []\n",
    "    \n",
    "    for item_idx, item in enumerate(data):\n",
    "        annotations, text = extract_annotations_with_text(item)\n",
    "        tokens, labels = create_ner_sequence(text, annotations, group_labels,tokenizer)\n",
    "        \n",
    "        if tokens and len(tokens) > 0:\n",
    "            dataset.append({\n",
    "                'tokens': tokens,\n",
    "                'ner_tags': labels\n",
    "            })\n",
    "    \n",
    "    # Сохраняем в JSONL\n",
    "    output_file = f'{group_name}_dataset.jsonl'\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for item in dataset:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "    \n",
    "    print(f\"Создан датасет {output_file} с {len(dataset)} примерами\")\n",
    "    \n",
    "    # Статистика\n",
    "    label_counts = defaultdict(int)\n",
    "    for item in dataset:\n",
    "        for tag in item['ner_tags']:\n",
    "            label_counts[tag] += 1\n",
    "    \n",
    "    print(f\"Статистика меток для {group_name}:\")\n",
    "    for label, count in sorted(label_counts.items()):\n",
    "        print(f\"   {label}: {count}\")\n",
    "    \n",
    "    return dataset, label_counts\n",
    "\n",
    "# Создаём датасеты\n",
    "dataset1, stat_1 = create_dataset_for_group(data, GROUP_1, \"group1\")\n",
    "dataset2, stat_2 = create_dataset_for_group(data, GROUP_2, \"group2\") \n",
    "dataset3, stat_3 = create_dataset_for_group(data, GROUP_3, \"group3\")\n",
    "statistics = [stat_1, stat_2, stat_3]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11847cfc-02ba-4903-9d8e-ca48676bf57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохранение статистики в pickle\n",
    "import pickle\n",
    "with open(FOLDER_PATH + '307_stat.pkl', 'wb') as f:\n",
    "    pickle.dump(statistics, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a223197-2eec-4d92-ab22-b888ff64c4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    150_resumes  307_resumes  difference relative difference\n",
      "O                        151246       310168      158922             105.08%\n",
      "B-TIME                     1759         3783        2024             115.07%\n",
      "I-TIME                     5771        13070        7299             126.48%\n",
      "B-LINKS                     787         1638         851             108.13%\n",
      "I-LINKS                   12599        22789       10190              80.88%\n",
      "B-POSITIONS                 850         1826         976             114.82%\n",
      "I-POSITIONS                3873         8522        4649             120.04%\n",
      "B-DEGREE                    255          559         304             119.22%\n",
      "I-DEGREE                    680         1481         801             117.79%\n",
      "B-LOCATION                  720         1599         879             122.08%\n",
      "I-LOCATION                  569         1117         548              96.31%\n",
      "B-METRICS                   201          425         224             111.44%\n",
      "I-METRICS                   974         1827         853              87.58%\n",
      "O                        157730       322924      165194             104.73%\n",
      "B-COMPANIES                 986         2105        1119             113.49%\n",
      "I-COMPANIES                4618        10251        5633             121.98%\n",
      "B-TECHNOLOGIES             6001        12007        6006             100.08%\n",
      "I-TECHNOLOGIES            10133        19284        9151              90.31%\n",
      "B-NAME                      177          437         260             146.89%\n",
      "I-NAME                      636         1787        1151             180.97%\n",
      "O                         84208       165902       81694              97.01%\n",
      "B-RESPONSIBILITIES         2710         5793        3083             113.76%\n",
      "I-RESPONSIBILITIES        48598       103769       55171             113.53%\n",
      "B-EDUCATION                 552         1151         599             108.51%\n",
      "I-EDUCATION                6275        14600        8325             132.67%\n",
      "B-SKILLS                    644         1194         550               85.4%\n",
      "I-SKILLS                   6747        14284        7537             111.71%\n",
      "B-PROJECTS                  427         1051         624             146.14%\n",
      "I-PROJECTS                11849        25200       13351             112.68%\n",
      "B-ACHIEVEMENTS              707         1303         596               84.3%\n",
      "I-ACHIEVEMENTS            17224        31530       14306              83.06%\n",
      "B-CONTACTS                   20          199         179              895.0%\n",
      "I-CONTACTS                  320         2828        2508             783.75%\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "with open(FOLDER_PATH + '150_stat.pkl', 'rb') as f_150:\n",
    "    stat_150 = pickle.load(f_150)\n",
    "with open(FOLDER_PATH + '307_stat.pkl', 'rb') as f_307:\n",
    "    stat_307 = pickle.load(f_307)\n",
    "def stat_to_pd(stat_data):\n",
    "    total_stat = list()\n",
    "    for stat in stat_data:\n",
    "        group_pd = pd.Series(stat)\n",
    "        total_stat.append(group_pd)\n",
    "    return pd.concat(total_stat)\n",
    "    \n",
    "stat_150_pd = stat_to_pd(stat_150)\n",
    "stat_307_pd = stat_to_pd(stat_307)\n",
    "total_stat_pd = pd.concat([stat_150_pd, stat_307_pd], axis = 1)\n",
    "total_stat_pd.columns = ['150_resumes', '307_resumes']\n",
    "difference = total_stat_pd['307_resumes'] - total_stat_pd['150_resumes']\n",
    "\n",
    "relative_difference = difference / total_stat_pd['150_resumes']\n",
    "relative_difference = relative_difference.apply(lambda x: str(round(x*100, 2)) + '%')\n",
    "total_stat_pd = pd.concat([total_stat_pd, difference, relative_difference], axis = 1)\n",
    "total_stat_pd.columns = ['150_resumes', '307_resumes', 'difference', 'relative difference']\n",
    "print(total_stat_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81aaf9f6-4141-4ec6-b293-705c1b17d544",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5639c70-671d-46fe-875f-c0460594d764",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
